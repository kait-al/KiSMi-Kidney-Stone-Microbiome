library(zCompositions) # CZM

library(dplyr) # Pipe

library(ggplot2) # Plotting

library(vegan) # envfit and diversity

library(ggsci) # Colours

library(ALDEx2) # ALDEx2

#### Load data ####
#count table = SV by column, samples by row
counts <- read.table("cutadapt_counts.txt", 
                     header = TRUE, row.names = 1, sep = "\t", check.names = FALSE, 
                     quote = "", stringsAsFactors = FALSE)
tax <- read.table("cutadapt_tax.txt", 
                  header = TRUE, row.names = 1, sep = "\t", check.names = FALSE, 
                  quote = "", stringsAsFactors = FALSE)

#### #load meta data table

metadata<-read.table("metadata_oral.txt", header=T, row.names = 1, sep='\t', comment.char = "")

#### Filter ####

# Summarize input counts table
sum(counts) ; dim(counts) ; summary(colSums(counts)) ; summary(rowSums(counts))

# [1] 8101148
# [1]  124 2541
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#       0       0      10    3188     113 3582585 
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#    8479   26649   53893   65332   84517  227471 



# Generate a relative abundance table and remove SVs accounting for < 1% of reads in every sample
props <- apply(counts, 1, function(x) {x/sum(x)})
filt_by_props <- counts[, apply(props, 1, max) >= 0.01] 

# calculate frequency minus the final (taxonomy) column
#d.freq <- apply(filt_by_props, 1, function(x){x/sum(x)})
#filtered_tax <- tax[colnames(filt_by_props), ]
#freq <- cbind(filtered_tax,d.freq)
#write.table(freq, file="cutadapt_proportions.txt", sep='\t')

# Summarize output filtered counts table
sum(filt_by_props) ; dim(filt_by_props) ; summary(colSums(filt_by_props)) ; summary(rowSums(filt_by_props))

# [1] 7774969
# [1] 124 164
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#     349    1783    4272   47408   16371 3582585 
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#    7515   25845   49766   62701   82909  227164 

# Create filtered data frames
filtered_counts <- counts[rownames(filt_by_props), colnames(filt_by_props)]
filtered_tax <- tax[colnames(filt_by_props), ]
filtered_meta <- metadata[rownames(filt_by_props), ]


#### PCA ####

# Replace zeros, transform, and perform PCA
czm <- cmultRepl(filtered_counts, label = 0, method = "CZM")
clr <- t(apply(czm, 1, function(x) {log(x) - mean(log(x))} ))
pca <- prcomp(clr)

#write.table(t(clr), file="cutadapt_counts_CLR.txt", sep='\t')
# Assemble PCA biplot
sample_positions <- data.frame(pca[["x"]])
asv_positions <- data.frame(pca[["rotation"]])

sample_plot_data <- merge(sample_positions, filtered_meta, by = 0)
rownames(sample_plot_data) <- sample_plot_data$Row.names
sample_plot_data$Row.names <- NULL

sample_plot_data$Cohort<-factor(sample_plot_data$Cohort, levels=c("hc","sf"))
cols <- c("hc" = "#492675", "sf" = "#EF8863")


pca_biplot <- ggplot() +
  geom_point(data = sample_plot_data, aes(x = PC1, y = PC2, colour=Cohort), size = 2.5) +
  stat_ellipse(data = sample_plot_data, aes(x = PC1, y = PC2, colour = Cohort), geom = "polygon", level = 0.95, alpha = 0.025, show.legend = NA, linetype = "dashed") +
  #scale_shape_manual(values = c(21, 24)) +
  scale_colour_manual(values = cols) +
  guides(fill = guide_legend(override.aes = list(shape = 21))) +
  geom_segment(data = asv_positions, aes(x = 0, y = 0, xend = 80 * PC1, yend = 80 * PC2), 
               arrow = arrow(length = unit(1/2, 'picas')),
               color = "black",
               alpha = 0.3) + 
  geom_text(data = asv_positions, aes(x = 80 * PC1, y = 80 * PC2), check_overlap = T,
            label = rownames(asv_positions),
            color = "darkgrey",
            size = 2.5) + 
  labs(x = paste("PC1: ", round(pca$sdev[1]^2/sum(pca$sdev^2),3)), 
       y = paste("PC2: ", round(pca$sdev[2]^2/sum(pca$sdev^2),3))) + 
  theme_linedraw() +
  theme(panel.grid = element_blank())
pca_biplot

fit1 <- envfit(pca, filtered_meta, permutations = 999, na.rm = TRUE)
fit1 #shows factor and categorical data

#aldex2
d<-t(filtered_counts)
hc<-c("H_450","H_451","H_452","H_453","H_454","H_455","H_456","H_457","H_458","H_459","H_459a","H_460","H_461","H_462","H_463","H_464","H_465","H_466","H_467","H_468","H_469","H_470","H_471","H_472","H_473","H_474","H_475","H_476","H_477","H_478","H_479")
sf<-c("S_101","S_102","S_103","S_104","S_105","S_106","S_107","S_107a","S_108","S_109","S_110","S_111","S_112","S_113","S_114","S_115","S_116","S_117","S_118","S_119","S_120","S_121","S_122","S_123","S_124","S_125","S_126","S_127","S_128","S_129","S_130","S_131","S_132","S_133","S_134","S_135","S_136","S_137","S_138","S_139","S_140","S_141","S_142","S_143","S_144","S_145","S_146","S_147","S_148","S_149","S_150","S_151","S_152","S_153","S_154","S_155","S_156","S_157","S_158","S_159","S_160","S_161","S_162","S_163","S_164","S_165","S_166","S_167","S_168","S_169","S_170","S_171","S_172","S_173","S_174","S_175","S_176","S_177","S_178","S_179","S_180","S_181","S_182","S_183","S_184")

#this will retain the same order as the lists above
# NOTE: ALDEx input must be a DATA FRAME *not* a matrix
aldex.in<-d[,c(hc,sf)]

#Make a vector of conditions. This must be in the same order and the same number as the columns (samples) of the input table (aldex.in)
conds<-c(rep("hc", length(hc)), rep("sf", length(sf)))

#get the clr values
#this is the main ALDEx function for all downstream analyses
#mc.samples=128 is often sufficient
x <- aldex.clr(aldex.in, conds, mc.samples=999, verbose=TRUE)

#perform t-test (both Welches and Wilcoxon, plus a Benjamini-Hochberg multiple test correction)
x.tt <- aldex.ttest(x, conds, paired.test=FALSE)

#estimate effect size and the within and between condition values
#include indiv. samples or not
x.effect <- aldex.effect(x, conds)

#merge the data
x.all <- data.frame(x.tt, x.effect)

#write a .txt with your results
write.table(x.all, file="aldex_hc_sf.txt", sep="\t", quote=F, col.names=NA)
