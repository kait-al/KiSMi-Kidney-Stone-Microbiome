d<-read.table("dada2_nochim_tax.txt", sep="\t", quote="", header=T, row.names=1)
dim(d)
#[1] 935 284



# can't use taxonomy column from data to calculate frequency (will give error)

tax<-d$tax.vector #by column name

# Filter samples, include only those >1000 reads
i <- (colSums(d[,1:ncol(d)-1]) <=1000)
d.s <- d[, !i]

dim(d.s)
#[1] 935 240
########## so went from 284 to 240 samples (44 removed)

# Keep only somewhat abundant stuff
# Remove SVs that dont have at least 10000 reads total (across all samples)
count = 10000
d.s2 <- data.frame(d.s[which(apply(d.s[,1:ncol(d.s)-1], 1, function(x){sum(x)}) > count),])

dim(d.s2)
#[1]  83 240
########## so went from 935 to 83 SVs (852 removed)

################
# calculate frequency minus the final (taxonomy) column
d.freq <- apply(d.s2[,1:ncol(d.s2)-1], 2, function(x){x/sum(x)})

# filter OTUs
# Keep OTUs with frequency of > 0.01 in any sample
# Keep the OTU as long as the maximum frequency of the OTU in any one sample is greater than the cutoff
d.0 <- d.s2[apply(d.freq, 1, max)>0.01,]

#check the dimensions, number of OTUs (rows) has been cut way down
dim(d.0)
# same as above
#[1]  83 240

#write filtered table to use downstream
write.table(d.0, file="filtSV_01_1000.txt", sep="\t", quote=F)



#### dims went from 935 SVs/ 277 samples to 83 SVs / 233 samples
#(samples >1000 reads only, OTUs of min.prop 1% in any sample), and only OTUs with >10,000 reads total (across any/all samples, from both miseq runs)
